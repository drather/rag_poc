# 2025년 8월 9일 RAG PoC 시스템 개선 및 디버깅 요약

## 1. 더미 문서 추가 및 반영
- 기존의 간단한 문서 외에, 시스템 구성, 보안 조치, 인프라 변경, API 연동, 신규 시스템 개발, 로직 변경 등 **업무 관련 상세 더미 문서 6종을 추가**했습니다.
- 이 문서들을 `rag_poc/chunks` 디렉토리에 생성하고, 임베딩 및 벡터 DB에 반영했습니다.

## 2. LangChain 통합 및 코드 리팩토링
- RAG 파이프라인을 LangChain 프레임워크 기반으로 전면 리팩토링했습니다.
- `rag_poc.py`와 `test_rag.py`에서 LangChain의 `SentenceTransformerEmbeddings`, `Chroma`, `HuggingFacePipeline`, `RetrievalQA` 체인 등을 활용하도록 코드를 변경했습니다.

## 3. 주요 디버깅 및 문제 해결

### 3.1. ChromaDB 영속성 문제 최종 해결
- **문제**: `DB에서 로드된 문서 개수: 0` 오류가 지속적으로 발생하여 데이터베이스가 제대로 로드되지 않았습니다.
- **원인**: 상대 경로 문제, `chromadb.PersistentClient`와 `collection.add`의 불완전한 영속화, `vector_db.py`의 예외 처리 미흡 등이 복합적으로 작용했습니다.
- **해결**: 
    - ChromaDB 경로를 **절대 경로(`D:\workspace\rag_poc\chroma_db`)로 명시**했습니다.
    - `vector_db.py`에서 `Chroma.from_documents` 메서드를 사용하여 **문서들을 직접 벡터스토어에 생성 및 영속화**하도록 변경했습니다. 이는 더 안정적인 DB 구축을 보장합니다.
    - `vector_db.py`의 예외 처리를 `chromadb.errors.NotFoundError`를 포함하도록 수정했습니다.

### 3.2. GPU 활용 및 성능 문제 해결
- **문제**: LLM이 CPU로만 실행되어 속도가 매우 느리고 `bitsandbytes` 관련 오류가 발생했습니다.
- **원인**: PyTorch가 CUDA 지원 없이 CPU 전용으로 설치되어 있었습니다 (`torch.version.cuda`가 `None`).
- **해결**: 
    - **CUDA 지원 PyTorch를 재설치**했습니다 (`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`).
    - `AutoModelForCausalLM.from_pretrained` 호출 시 `quantization_config=BitsAndBytesConfig(load_in_8bit=True)`를 다시 활성화하여 **GPU를 통한 8비트 양자화 로드**를 가능하게 했습니다.
    - `pipeline` 생성 시 `device` 인수를 제거하여 `accelerate`가 장치 할당을 관리하도록 했습니다.

### 3.3. LLM 답변 품질 및 환각 문제 개선
- **문제**: LLM이 질문에 대해 엉뚱한 답변을 하거나, 문서에 없는 내용을 지어내는(환각) 현상이 심했습니다.
- **원인**: 
    - 초기 임베딩 모델(`jhgan/ko-sroberta-multitask`)의 검색 정확도 한계.
    - `k` 값(검색 문서 수)의 부적절한 설정.
    - LLM(`KoAlpaca-Polyglot-5.8B`)의 지시 추종 능력 한계.
- **해결**: 
    - **임베딩 모델을 `snunlp/KR-SBERT-V40K-klueNLI-augSTS`로 교체**하여 검색 정확도를 높였습니다.
    - **`k` 값을 2 또는 3으로 조정**하여 LLM에 더 풍부한 컨텍스트를 제공했습니다.
    - **사용자 정의 `PromptTemplate`을 적용**하여 LLM에 더 명확한 지시를 전달했습니다.
    - **LLM 직접 추론 능력 테스트를 추가**하여, LLM이 직접 주어진 정보에서는 추론 능력이 있음을 확인했습니다. 이는 RAG의 병목이 검색 또는 컨텍스트 처리 방식에 있음을 시사했습니다.

## 4. 최종 결과
- **GPU 활용 및 성능:** 성공적으로 GPU를 사용하여 LLM 로드 및 추론 속도를 크게 향상시켰습니다.
- **ChromaDB 및 LangChain 통합:** 모든 문서가 올바르게 로드되고, LangChain 프레임워크를 통해 RAG 파이프라인이 안정적으로 작동합니다.
- **RAG 답변 품질:** 새로운 더미 문서, 임베딩 모델, `k` 값 조정, 그리고 프롬프트 엔지니어링을 통해 **대부분의 질문에 대해 정확하고 관련성 있는 답변을 제공**하며, 환각 현상이 크게 줄었습니다.
- **PoC 목표 달성:** 로컬 환경에서 경량 모델을 활용한 RAG 시스템의 핵심 기술 검증 목표를 성공적으로 달성했습니다.

이것으로 오늘 진행된 모든 작업과 그 결과에 대한 요약이 완료되었습니다.