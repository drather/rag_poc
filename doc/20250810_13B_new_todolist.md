# 13B 모델 도입 및 RAG 고도화 TODO List

> 5.8B 모델의 환각(Hallucination) 현상을 해결하고, 더 정확한 답변을 제공하기 위해 13B 모델을 양자화하여 도입하는 것을 목표로 합니다.

---

### 1. 기반 모델 선정 (13B) - ✅ 완료

- [x] **Hugging Face 모델 탐색**: `KoAlpaca`, `KoSOLAR` 등 후보군 비교 분석 완료.
- [x] **라이선스 확인**: 최종 후보 모델들이 모두 Apache 2.0 라이선스임을 확인.
- **최종 결정 (2025-08-10, 변경):**
  - **모델**: `beomi/KoAlpaca-Polyglot-12.8b` (GGUF 버전)
  - **선정 이유**: `KoSOLAR`는 사전 양자화된 GGUF 모델이 없어 직접 변환하는 작업이 필요. 안정성과 개발 속도를 고려하여, 커뮤니티에서 검증된 GGUF 모델이 존재하는 `KoAlpaca`로 변경.
  - **양자화 방식**: 로컬 환경(VRAM 8GB, RAM 32GB)에 최적화된 **GGUF** 방식 사용.

### 2. 모델 양자화 (Quantization) - ✅ 완료

- [x] **양자화 방식 결정**: 사전 양자화된 GGUF 모델을 사용하기로 결정.
- [x] **양자화 스크립트 작성 또는 도구 준비**: 별도 준비 필요 없음.
- [x] **모델 다운로드 및 양자화 실행**: `TheBloke/KoAlpaca-Polyglot-12.8B-GGUF` 저장소에서 `Q4_K_M` 버전 모델 다운로드 완료.

### 3. RAG 파이프라인 연동

- [x] **`requirements.txt` 업데이트**: `llama-cpp-python` 라이브러리 추가 및 설치 완료.
  - **트러블슈팅 기록**:
    - `pip install` 시 C++ 컴파일러 부재로 빌드 실패가 반복적으로 발생.
    - Pre-built wheel URL을 이용한 설치 방법은 URL의 변경/삭제로 인해 실패.
    - 환경 변수(CMAKE_ARGS)를 통한 빌드 역시, 기본 터미널에서는 컴파일러 경로를 인지하지 못해 실패.
    - **해결**: Visual Studio Build Tools에서 **"C++를 사용한 데스크톱 개발"** 워크로드를 설치하고, **"Developer Command Prompt for VS"**를 통해 `pip install llama-cpp-python`을 실행하여 성공적으로 설치 완료.
- [ ] **모델 로더(LLM) 교체**: `rag_poc.py` 코드에서 기존 5.8B 모델을 로드하는 부분을, 양자화된 13B 모델을 로드하는 코드로 수정.
  - `LlamaCpp` 등 양자화 방식에 맞는 LangChain 또는 Transformers 로더 사용.
- [ ] **프롬프트 템플릿 수정**: 새로운 모델의 특성에 맞게, 컨텍스트와 질문을 전달하는 프롬프트 템플릿을 최적화. (필요시)

### 4. 성능 테스트 및 평가

- [ ] **기존 실패 케이스 테스트**: 5.8B 모델에서 환각 현상이 발생했던 질문들을 다시 실행하여, 13B 모델이 정확한 답변을 생성하는지 확인.
- [ ] **성능 비교 분석**: 답변의 정확성, 속도, 리소스 사용량 등을 정량적/정성적으로 비교하여 모델 교체의 효과를 검증.
- [ ] **결과 정리**: 테스트 결과와 최종 PoC 성과를 문서화.

---
