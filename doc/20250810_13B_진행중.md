# 2025년 8월 10일: 13B 모델 도입 진행 상황

## 목표
- RAG 시스템의 LLM을 `beomi/KoAlpaca-Polyglot-5.8B`에서 `qwopqwop/KoAlpaca-Polyglot-12.8B-GPTQ` (13B) 모델로 변경하여 답변 품질 향상.

## 수행 내역

### 1. `auto-gptq` 라이브러리 설치 시도 및 문제 해결
- **문제 1**: `pip install auto-gptq` 실패 (`No module named 'torch'` 오류).
    - **원인**: `auto-gptq` 빌드 과정에서 `torch` 모듈을 찾지 못함. PyTorch가 CUDA 지원 없이 설치되었을 가능성.
    - **해결**: 
        - `torch`가 CUDA 지원으로 설치되었는지 확인 (`python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"`).
        - `torch.cuda.is_available()`이 `False`로 확인되어, **CPU 전용 PyTorch가 설치되어 있었음을 파악**.
        - 기존 PyTorch 제거 후, **CUDA 11.8 지원 PyTorch 재설치** (`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`).
- **문제 2**: `pip install auto-gptq --no-build-isolation` 실패 (`CUDA_HOME environment variable is not set` 오류).
    - **원인**: `auto-gptq` 빌드 시 `CUDA_HOME` 환경 변수가 설정되지 않아 CUDA Toolkit 경로를 찾지 못함.
    - **해결**: PowerShell에서 `CUDA_HOME` 환경 변수를 CUDA Toolkit 설치 경로로 설정 (`$env:CUDA_HOME="C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8"`).
- **문제 3**: `pip install auto-gptq --no-build-isolation` 실패 (`CUDA_PATH environment variable must be set` 오류).
    - **원인**: `auto-gptq` 빌드 시 `CUDA_PATH` 환경 변수도 설정되어야 함.
    - **해결**: PowerShell에서 `CUDA_PATH` 환경 변수를 CUDA Toolkit 설치 경로로 설정 (`$env:CUDA_PATH="C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8"`).
- **결과**: `auto-gptq` 라이브러리 설치 완료.

### 2. LLM 모델 변경 및 로드 시도
- **모델 ID 변경**: `llm_model_id`를 `TheBloke/KoAlpaca-Polyglot-12.8B-GPTQ`로 변경.
- **문제 1**: `TheBloke/KoAlpaca-Polyglot-12.8B-GPTQ` 로드 실패 (`404 Client Error: Not Found` for `tokenizer_config.json`).
    - **원인**: `TheBloke` 모델 저장소에 토크나이저 파일이 직접 없거나, `AutoTokenizer`가 기본 `main` 브랜치에서 찾지 못함.
    - **해결**: 토크나이저를 **원본 모델인 `beomi/KoAlpaca-Polyglot-12.8B`에서 로드**하도록 변경.
- **문제 2**: 모델 로드 실패 (`OSError: ... does not appear to have a file named pytorch_model.bin, model.safetensors...`).
    - **원인**: `AutoGPTQForCausalLM`이 기본 모델 가중치 파일 이름(`pytorch_model.bin` 등)을 찾지 못함. `TheBloke` 모델은 `gptq_model-4bit-128g.bin`과 같은 다른 이름을 사용.
    - **해결**: `AutoGPTQForCausalLM.from_pretrained` 호출 시 `model_basename="gptq_model-4bit-128g.bin"` 인수를 추가하여 모델 파일 이름을 명시.
- **문제 3**: 모델 로드 실패 (`OSError: ... cannot be loaded with `safetensors`. Please make sure ... or do not set `use_safetensors=True`.`).
    - **원인**: 모델이 `safetensors` 형식으로 저장되지 않았는데 `use_safetensors=True`로 설정됨.
    - **해결**: `use_safetensors=False`로 변경.
- **문제 4**: 모델 로드 실패 (`OSError: ... does not appear to have a file named pytorch_model.bin, model.safetensors...` 오류 재발).
    - **원인**: `model_basename`을 명시했음에도 `transformers`가 기본 파일 이름을 찾음. `AutoGPTQForCausalLM`의 로딩 방식에 대한 더 깊은 이해 필요.
    - **해결 시도**: `model_file="gptq_model-4bit-128g.bin"` 인수를 추가하여 모델 가중치 파일을 명시적으로 지정. `use_marlin=False` 추가.
- **문제 5**: `TheBloke/KoAlpaca-Polyglot-12.8B-GPTQ` 모델 ID 자체의 문제.
    - **원인**: 웹 검색 결과, `TheBloke/KoAlpaca-Polyglot-12.8B-GPTQ` 모델은 Hugging Face Hub에 **존재하지 않는 모델 ID**였음. (이전 `404` 오류의 근본 원인).
    - **해결**: **`llm_model_id`를 `qwopqwop/KoAlpaca-Polyglot-12.8B-GPTQ`로 변경**.
    - **토크나이저 로드 방식 복원**: `qwopqwop` 모델은 자체 토크나이저 파일을 가지고 있으므로, 토크나이저도 `llm_model_id`에서 로드하도록 변경.
- **문제 6**: `AutoGPTQForCausalLM` 로드 시 `quantize_config=None` 문제.
    - **원인**: `AutoGPTQForCausalLM`은 `quantize_config` 인수에 `GPTQConfig()` 객체를 기대함.
    - **해결**: `quantize_config=GPTQConfig()`로 변경.

## 현재 상태
- `qwopqwop/KoAlpaca-Polyglot-12.8B-GPTQ` 모델 로드를 시도 중이며, 마지막으로 `model_file` 인수를 명시하고 `trust_remote_code=True`를 추가한 상태에서 `OSError`가 재발했습니다. 이는 `AutoGPTQForCausalLM`이 `model_file` 인수를 올바르게 사용하지 못하거나, 모델의 `config.json`이 `model_basename`을 명시하지 않아 발생하는 문제입니다.

## 다음 단계 (새로운 세션에서 시작)
- `qwopqwop/KoAlpaca-Polyglot-12.8B-GPTQ` 모델의 Hugging Face Hub 페이지를 다시 한번 면밀히 검토하여 정확한 로드 지침을 확인.
- `AutoGPTQForCausalLM` 로드 시 `model_file` 인수가 제대로 작동하지 않는 문제에 대한 추가 디버깅.
- 필요시 `GPTQConfig` 객체를 명시적으로 생성하여 전달하는 방법 재시도.

이 문서를 바탕으로 새로운 세션에서 작업을 이어갈 예정입니다.